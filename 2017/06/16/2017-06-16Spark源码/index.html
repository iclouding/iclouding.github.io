<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Spark," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="Spark 源码整体流程 使用Spark Submit脚本提交spark 作业，会启动Driver进程执行我们的Application应用程序。 Driver会初始化SparkContext，此时，会构造出DAGScheduler和TaskScheduler。 在构造TaskScheduler事，会去连接Master，向Master注册APP Master在收到Application的注册请求后，">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark 源码">
<meta property="og:url" content="https://iclouding.github.io/2017/06/16/2017-06-16Spark源码/index.html">
<meta property="og:site_name" content="大数据部落">
<meta property="og:description" content="Spark 源码整体流程 使用Spark Submit脚本提交spark 作业，会启动Driver进程执行我们的Application应用程序。 Driver会初始化SparkContext，此时，会构造出DAGScheduler和TaskScheduler。 在构造TaskScheduler事，会去连接Master，向Master注册APP Master在收到Application的注册请求后，">
<meta property="og:updated_time" content="2017-12-08T05:13:49.226Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark 源码">
<meta name="twitter:description" content="Spark 源码整体流程 使用Spark Submit脚本提交spark 作业，会启动Driver进程执行我们的Application应用程序。 Driver会初始化SparkContext，此时，会构造出DAGScheduler和TaskScheduler。 在构造TaskScheduler事，会去连接Master，向Master注册APP Master在收到Application的注册请求后，">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://iclouding.github.io/2017/06/16/2017-06-16Spark源码/"/>





  <title>Spark 源码 | 大数据部落</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">大数据部落</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Hadoop, Spark, OpenStack，Docker</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://iclouding.github.io/2017/06/16/2017-06-16Spark源码/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="iclouding">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大数据部落">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark 源码</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-16T00:00:00+08:00">
                2017-06-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2017-12-08T13:13:49+08:00">
                2017-12-08
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a class="cloud-tie-join-count" href="/2017/06/16/2017-06-16Spark源码/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count join-count" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/06/16/2017-06-16Spark源码/" class="leancloud_visitors" data-flag-title="Spark 源码">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="Spark-源码"><a href="#Spark-源码" class="headerlink" title="Spark 源码"></a>Spark 源码</h1><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><ol>
<li>使用Spark Submit脚本提交spark 作业，会启动Driver进程执行我们的Application应用程序。</li>
<li>Driver会初始化SparkContext，此时，会构造出DAGScheduler和TaskScheduler。</li>
<li>在构造TaskScheduler事，会去连接Master，向Master注册APP</li>
<li>Master在收到Application的注册请求后，会使用一定的调度算法，在spark集群上为Application启动Worker</li>
<li>Master会通知Worker去启动Executor</li>
<li>Executor在启动完毕后，会反向注册到TaskScheduler上去。</li>
<li>所有Executor都注册到TaskScheduler之后，SparkContext初始化完毕。然后会继续执行我们的代码逻辑</li>
<li>此时，代码每执行到一个Action操作，就会创建一个Job，把Job提交给DAGScheduler，DAGScheduler会将job划分成多个stage，为每个stage创建一个TaskSet。（此处有很重要的Stage的划分算法）</li>
<li>TaskScheduler会将每个taskset提交到Executor上执行。（task分配算法）</li>
<li>Executor每接收到一个task，就会使用TaskRunner来封装task，然后从线程池中取出一个线程，来运行这个task。TaskRunner将我们编写的代码，算法及函数进行反序列化，然后执行task。</li>
</ol>
<p>Task其实有两种：ShuffleMapTask和ResultTask，job中最后一个是ResultTask，其他的都是ShuffleMapTask</p>
<ol>
<li>最后，每个task会针对rdd中的某个partition，执行我们的算子。</li>
</ol>
<h2 id="SparkContext分析"><a href="#SparkContext分析" class="headerlink" title="SparkContext分析"></a>SparkContext分析</h2><p>SparkContext初始化后，会创建TaskScheduler，此时，</p>
<ol>
<li>实例化TaskSchedulerImpl对象</li>
<li>实例化SparkDeploySchedulerBackend，在底层接受TaskScheduler的控制，负责Master注册、Executor反注册，task发送到Executor等操作</li>
<li>创建SchedulerPool，有不同的优先策略，比如FIFO，FAIR等</li>
<li>之后会启动TaskSchedulerImpl的start方法，此时会创建一个ClientActor，向master进行注册tryRegisterAllMasters，里面封装了Application的信息</li>
<li>此后，Spark集群中Master会通知worker启动Executor，并且反向注册到SparkDeploySchedulerBackend上</li>
</ol>
<p>此外，还有一个DAGScheduler，底层会基于DAGSchedulerEventProcessActor通信；SparkUI，通过启动jetty服务器，来提供web服务。</p>
<hr>
<p>TaskScheduler作用</p>
<ol>
<li>底层操作一个SchedulerBackend，针对不同种类的cluter，创建不同的backend</li>
<li>负责一些通用逻辑的，比如，job调度顺序，启动推测执行等</li>
<li>客户端首先会调用initialize()方法，和start()方法，然后通过runTasks()提交task sets</li>
</ol>
<p>有个很重要的就是ApplicationDescription的创建，是用于描述app的信息，代表了当前Application的需要的cpu、Executor等信息。创建完ApplicationDescription之后，会创建一个AppClient，AppClient是一个接口，<strong>负责application与Spark集群的通信</strong>，接收一个spark master的url，以及一个ApplicationDescription，一个集群事件的监听器以及对应事件发生的时候的回调函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createTaskScheduler</span></span>(</div><div class="line">      sc: <span class="type">SparkContext</span>,</div><div class="line">      master: <span class="type">String</span>): (<span class="type">SchedulerBackend</span>, <span class="type">TaskScheduler</span>) = &#123;</div><div class="line">    <span class="keyword">import</span> <span class="type">SparkMasterRegex</span>._</div><div class="line"></div><div class="line">    <span class="comment">// When running locally, don't try to re-execute tasks on failure.</span></div><div class="line">    <span class="keyword">val</span> <span class="type">MAX_LOCAL_TASK_FAILURES</span> = <span class="number">1</span></div><div class="line"></div><div class="line">    master <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="string">"local"</span> =&gt;</div><div class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)</div><div class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalBackend</span>(sc.getConf, scheduler, <span class="number">1</span>)</div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">LOCAL_N_REGEX</span>(threads) =&gt;</div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()</div><div class="line">        <span class="comment">// local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.</span></div><div class="line">        <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">"*"</span>) localCpuCount <span class="keyword">else</span> threads.toInt</div><div class="line">        <span class="keyword">if</span> (threadCount &lt;= <span class="number">0</span>) &#123;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Asked to run locally with <span class="subst">$threadCount</span> threads"</span>)</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)</div><div class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalBackend</span>(sc.getConf, scheduler, threadCount)</div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">LOCAL_N_FAILURES_REGEX</span>(threads, maxFailures) =&gt;</div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()</div><div class="line">        <span class="comment">// local[*, M] means the number of cores on the computer with M failures</span></div><div class="line">        <span class="comment">// local[N, M] means exactly N threads with M failures</span></div><div class="line">        <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">"*"</span>) localCpuCount <span class="keyword">else</span> threads.toInt</div><div class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, maxFailures.toInt, isLocal = <span class="literal">true</span>)</div><div class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalBackend</span>(sc.getConf, scheduler, threadCount)</div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">SPARK_REGEX</span>(sparkUrl) =&gt;</div><div class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)</div><div class="line">        <span class="keyword">val</span> masterUrls = sparkUrl.split(<span class="string">","</span>).map(<span class="string">"spark://"</span> + _)</div><div class="line">      <span class="comment">// 主要是这里</span></div><div class="line">        <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">SparkDeploySchedulerBackend</span>(scheduler, sc, masterUrls)</div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="string">"yarn-standalone"</span> | <span class="string">"yarn-cluster"</span> =&gt;</div><div class="line">        <span class="keyword">if</span> (master == <span class="string">"yarn-standalone"</span>) &#123;</div><div class="line">          logWarning(</div><div class="line">            <span class="string">"\"yarn-standalone\" is deprecated as of Spark 1.0. Use \"yarn-cluster\" instead."</span>)</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(<span class="string">"org.apache.spark.scheduler.cluster.YarnClusterScheduler"</span>)</div><div class="line">          <span class="keyword">val</span> cons = clazz.getConstructor(classOf[<span class="type">SparkContext</span>])</div><div class="line">          cons.newInstance(sc).asInstanceOf[<span class="type">TaskSchedulerImpl</span>]</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="comment">// <span class="doctag">TODO:</span> Enumerate the exact reasons why it can fail</span></div><div class="line">          <span class="comment">// But irrespective of it, it means we cannot proceed !</span></div><div class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"YARN mode not available ?"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">val</span> backend = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> clazz =</div><div class="line">            <span class="type">Utils</span>.classForName(<span class="string">"org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend"</span>)</div><div class="line">          <span class="keyword">val</span> cons = clazz.getConstructor(classOf[<span class="type">TaskSchedulerImpl</span>], classOf[<span class="type">SparkContext</span>])</div><div class="line">          cons.newInstance(scheduler, sc).asInstanceOf[<span class="type">CoarseGrainedSchedulerBackend</span>]</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"YARN mode not available ?"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="string">"yarn-client"</span> =&gt;</div><div class="line">        <span class="keyword">val</span> scheduler = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(<span class="string">"org.apache.spark.scheduler.cluster.YarnScheduler"</span>)</div><div class="line">          <span class="keyword">val</span> cons = clazz.getConstructor(classOf[<span class="type">SparkContext</span>])</div><div class="line">          cons.newInstance(sc).asInstanceOf[<span class="type">TaskSchedulerImpl</span>]</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"YARN mode not available ?"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">val</span> backend = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> clazz =</div><div class="line">            <span class="type">Utils</span>.classForName(<span class="string">"org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend"</span>)</div><div class="line">          <span class="keyword">val</span> cons = clazz.getConstructor(classOf[<span class="type">TaskSchedulerImpl</span>], classOf[<span class="type">SparkContext</span>])</div><div class="line">          cons.newInstance(scheduler, sc).asInstanceOf[<span class="type">CoarseGrainedSchedulerBackend</span>]</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"YARN mode not available ?"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line">      <span class="comment">// 这里还有很多，省略了。。</span></div><div class="line">      <span class="keyword">case</span> _ =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Could not parse Master URL: '"</span> + master + <span class="string">"'"</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>主要是根据正则匹配，创建不通的<code>SparkDeploySchedulerBackend</code>，之后initialize</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(backend: <span class="type">SchedulerBackend</span>) &#123;</div><div class="line">  <span class="keyword">this</span>.backend = backend</div><div class="line">  <span class="comment">// temporarily set rootPool name to empty</span></div><div class="line">  rootPool = <span class="keyword">new</span> <span class="type">Pool</span>(<span class="string">""</span>, schedulingMode, <span class="number">0</span>, <span class="number">0</span>)</div><div class="line">  schedulableBuilder = &#123;</div><div class="line">    schedulingMode <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</div><div class="line">        <span class="keyword">new</span> <span class="type">FIFOSchedulableBuilder</span>(rootPool)</div><div class="line">      <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</div><div class="line">        <span class="keyword">new</span> <span class="type">FairSchedulableBuilder</span>(rootPool, conf)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  schedulableBuilder.buildPools()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>initialize方法会创建不同的调度策略，目前有两种，FIFO和FAIR。后面再看一下这个的细节。</p>
<p>之后就是start方法</p>
<p>SparkDeploySchedulerBackend的start方法，最开始会准备一些jar包，环境变量，Executor个数，内存，cpu等等信息，关键是创建了一个ApplicationDescription实例，代表这个app信息，然后创建了一个Appclient，用来与master通信。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> appDesc = <span class="keyword">new</span> <span class="type">ApplicationDescription</span>(sc.appName, maxCores, sc.executorMemory,</div><div class="line">  command, appUIAddress, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor)</div><div class="line">client = <span class="keyword">new</span> <span class="type">AppClient</span>(sc.env.rpcEnv, masters, appDesc, <span class="keyword">this</span>, conf)</div><div class="line">client.start()</div><div class="line">launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">SUBMITTED</span>)</div><div class="line">waitForRegistration()</div><div class="line">launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">RUNNING</span>)</div></pre></td></tr></table></figure>
<p>start方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">  <span class="comment">// Just launch an rpcEndpoint; it will call back into the listener.</span></div><div class="line">  endpoint.set(rpcEnv.setupEndpoint(<span class="string">"AppClient"</span>, <span class="keyword">new</span> <span class="type">ClientEndpoint</span>(rpcEnv)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>// TODO</p>
<p>这个地方，对于endpoint我还是不清楚，后面要补一下。</p>
<p>在启动过程中，会调用onStart()方法，这个会调用<code>registerWithMaster(1)</code>，1用统计注册的次数，如果次数太多，则报错。最终会调用到<code>tryRegisterAllMasters</code>，对所有的进行注册</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">tryRegisterAllMasters</span></span>(): <span class="type">Array</span>[<span class="type">JFuture</span>[_]] = &#123;</div><div class="line">  <span class="keyword">for</span> (masterAddress &lt;- masterRpcAddresses) <span class="keyword">yield</span> &#123;</div><div class="line">    registerMasterThreadPool.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</div><div class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">if</span> (registered.get) &#123;</div><div class="line">          <span class="keyword">return</span></div><div class="line">        &#125;</div><div class="line">        logInfo(<span class="string">"Connecting to master "</span> + masterAddress.toSparkURL + <span class="string">"..."</span>)</div><div class="line">        <span class="keyword">val</span> masterRef =</div><div class="line">          rpcEnv.setupEndpointRef(<span class="type">Master</span>.<span class="type">SYSTEM_NAME</span>, masterAddress, <span class="type">Master</span>.<span class="type">ENDPOINT_NAME</span>)</div><div class="line">        masterRef.send(<span class="type">RegisterApplication</span>(appDescription, self))</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// Cancelled</span></div><div class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">s"Failed to connect to master <span class="subst">$masterAddress</span>"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这其中，最关键的就是向master发送了RegisterApplication的消息，master接手后会对app进行注册。</p>
<hr>
<p>还有一个很重要的就是DAGScheduler</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create and start the scheduler</span></div><div class="line"><span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master)</div><div class="line">_schedulerBackend = sched</div><div class="line">_taskScheduler = ts</div><div class="line">_dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)</div><div class="line">_heartbeatReceiver.ask[<span class="type">Boolean</span>](<span class="type">TaskSchedulerIsSet</span>)</div><div class="line"><span class="comment">// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's</span></div><div class="line"><span class="comment">// constructor</span></div><div class="line">_taskScheduler.start()</div></pre></td></tr></table></figure>
<ol>
<li>DAGScheduler实现了面向Stage的高层次的调度层，会为每个job计算一个stage的DAG，追踪RDD和stage是否被物化，并寻找一个最少消耗的机制来运行job</li>
<li>会将stage作为tasksets提交到底层的TaskSchedulerImpl上，在集群运行task</li>
<li>还决定运行每个task的最佳位置，基于当前的缓存状态，将最佳位置提交给底层的TaskSchedulerImpl，此外，还负责处理shuffle输出文件丢失导致的失败。这时，该stage会被重新提交。但是，如果一个stage的内部失败，如果不是由于shuffle文件丢失导致的，会被TaskScheduler处理（比如OOM），会多次重试每一个task。实在不行，才会去取消整个stage</li>
</ol>
<hr>
<h2 id="Master原理"><a href="#Master原理" class="headerlink" title="Master原理"></a>Master原理</h2><p>master主要负责</p>
<ol>
<li>主备切换</li>
<li>app的注册</li>
<li>状态改变</li>
<li>资源调度</li>
</ol>
<p>Spark默认的Standalone模式，是支持master高可用的。默认有两种实现：基于文件系统的和基于Zookeeper机制的。基于文件系统的主备切换，要在active master挂了，手动去standby 上启动。而基于Zookeeper会自动实现主备切换。</p>
<p>注册：master启动后，会接受来自worker、driver和application的注册，注册之后，会对application进行统一调度。</p>
<p>资源调度机制：scheduler()方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">schedule</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// 首先判断master状态，如果是standby，直接退出</span></div><div class="line">    <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;</div><div class="line">      <span class="keyword">return</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">// Drivers take strict precedence over executors</span></div><div class="line">    <span class="comment">// 首先，打散worker，shuffle函数就是对传入的集合元素进行随机打乱</span></div><div class="line">    <span class="keyword">val</span> shuffledAliveWorkers = <span class="type">Random</span>.shuffle(workers.toSeq.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>))</div><div class="line">    <span class="comment">// shuffledAliveWorkers就是所有打散了的活着的worker节点</span></div><div class="line">    <span class="keyword">val</span> numWorkersAlive = shuffledAliveWorkers.size</div><div class="line">    <span class="keyword">var</span> curPos = <span class="number">0</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">    *  这里首先调度driver</div><div class="line">    *  只有使用yarn-cluster模式提交的时候，才需要调度driver。如果使用yarn-client和standalone模式，</div><div class="line">    *  默认driver会启动在本地，而不需要master的调度</div><div class="line">    **/</div><div class="line">    </div><div class="line">    <span class="comment">// 遍历driver，waitingDrivers是一个ArrayBuffer</span></div><div class="line">    <span class="keyword">for</span> (driver &lt;- waitingDrivers.toList) &#123; <span class="comment">// iterate over a copy of waitingDrivers</span></div><div class="line"> <span class="comment">// We assign workers to each waiting driver in a round-robin fashion. For each driver, we</span></div><div class="line"> <span class="comment">// start from the last worker that was assigned a driver, and continue onwards until we have</span></div><div class="line"> <span class="comment">// explored all alive workers.</span></div><div class="line">      <span class="keyword">var</span> launched = <span class="literal">false</span></div><div class="line">      <span class="keyword">var</span> numWorkersVisited = <span class="number">0</span></div><div class="line">      </div><div class="line"><span class="comment">// 如果numWorkersVisited &lt; numWorkersAlive, 也就是说，活着的worker还没有被遍历到，那么就进行遍历</span></div><div class="line">      <span class="comment">//并且，这个driver还没有被启动</span></div><div class="line">      <span class="keyword">while</span> (numWorkersVisited &lt; numWorkersAlive &amp;&amp; !launched) &#123;</div><div class="line">        <span class="keyword">val</span> worker = shuffledAliveWorkers(curPos)</div><div class="line">        numWorkersVisited += <span class="number">1</span></div><div class="line">        </div><div class="line">        <span class="comment">// 此处判断，如果该worker的空闲内存大于等于driver的内存，并且worker的空闲cpu大于driver的</span></div><div class="line">        <span class="comment">// 剩余cpu，那么就启动driver</span></div><div class="line">        <span class="keyword">if</span> (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123;</div><div class="line">          launchDriver(worker, driver)</div><div class="line">          waitingDrivers -= driver</div><div class="line">          launched = <span class="literal">true</span></div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 将指针指向下一个worker</span></div><div class="line">        curPos = (curPos + <span class="number">1</span>) % numWorkersAlive</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 为application调度资源</span></div><div class="line">    startExecutorsOnWorkers()</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>master的Executor调度算法</p>
<p>Spark默认为应用程序启动Executor的方式是FIFO的方式，也就是所有提交的应用程序都是放在调试的等待队列中的，先进先出，只有满足了前面应用程序的资源分配的基础上才能够满足下一个应用程序资源的分配。</p>
<p>launchDriver如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchDriver</span></span>(worker: <span class="type">WorkerInfo</span>, driver: <span class="type">DriverInfo</span>) &#123;</div><div class="line">  logInfo(<span class="string">"Launching driver "</span> + driver.id + <span class="string">" on worker "</span> + worker.id)</div><div class="line">  <span class="comment">// 将driver加入到worker的缓存结构中</span></div><div class="line">  <span class="comment">// 将worker使用的内存和cpu数量，都加上driver需要的cpu和内存</span></div><div class="line">  worker.addDriver(driver)</div><div class="line">  <span class="comment">// 也把worker加入到了driver的内存缓存中</span></div><div class="line">  driver.worker = <span class="type">Some</span>(worker)</div><div class="line">  <span class="comment">// 向worker发送启动driver消息LaunchDriver</span></div><div class="line">  worker.endpoint.send(<span class="type">LaunchDriver</span>(driver.id, driver.desc))</div><div class="line">  driver.state = <span class="type">DriverState</span>.<span class="type">RUNNING</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>startExecutorsOnWorkers</code>是很重要的，在worker上启动Executor的算法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startExecutorsOnWorkers</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="comment">// Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app</span></div><div class="line">    <span class="comment">// in the queue, then the second app, etc.</span></div><div class="line">    </div><div class="line">    <span class="comment">// 为waitingApps中的app调度资源，app.coresLeft是app还有多少core没有分配。如果不需要则不会为应用程序分配Executor</span></div><div class="line">    <span class="keyword">for</span> (app &lt;- waitingApps <span class="keyword">if</span> app.coresLeft &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="keyword">val</span> coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>] = app.desc.coresPerExecutor</div><div class="line">      <span class="comment">// Filter out workers that don't have enough resources to launch an executor</span></div><div class="line">      </div><div class="line"> <span class="comment">// 筛选出状态为ALIVE并且这个worker剩余内存，剩余core都大于等于app的要求，然后按照coresFree降序排列</span></div><div class="line">      <span class="keyword">val</span> usableWorkers = workers.toArray.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>)</div><div class="line">        .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp;</div><div class="line">          worker.coresFree &gt;= coresPerExecutor.getOrElse(<span class="number">1</span>))</div><div class="line">        .sortBy(_.coresFree).reverse</div><div class="line">      </div><div class="line">      <span class="comment">// 在usableWorkers上为app分配Executor</span></div><div class="line">      <span class="comment">// assignedCores是一个数组，assignedCores[i]里面存储了需要在usableWorkers[i]上分配的core个数，譬如如果assingedCores[1]=2，那么就需要在usableWorkers[1]上分配2个core。</span></div><div class="line">      <span class="keyword">val</span> assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)</div><div class="line"></div><div class="line"><span class="comment">// Now that we've decided how many cores to allocate on each worker, let's allocate them</span></div><div class="line">      <span class="comment">// 在worker上启动Executor进程</span></div><div class="line">      <span class="keyword">for</span> (pos &lt;- <span class="number">0</span> until usableWorkers.length <span class="keyword">if</span> assignedCores(pos) &gt; <span class="number">0</span>) &#123;</div><div class="line">        allocateWorkerResourceToExecutors(</div><div class="line">          app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p><code>scheduleExecutorsOnWorkers</code>是用来调度Executor的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">scheduleExecutorsOnWorkers</span></span>(</div><div class="line">    app: <span class="type">ApplicationInfo</span>,</div><div class="line">    usableWorkers: <span class="type">Array</span>[<span class="type">WorkerInfo</span>],</div><div class="line">    spreadOutApps: <span class="type">Boolean</span>): <span class="type">Array</span>[<span class="type">Int</span>] = &#123;</div><div class="line">    <span class="comment">// 首先进行一系列初始化</span></div><div class="line">    <span class="comment">// 每个 executor 需要多少个 core</span></div><div class="line">  <span class="keyword">val</span> coresPerExecutor = app.desc.coresPerExecutor</div><div class="line">  <span class="comment">// 每个 executor 最少需要多少个 core，默认是 1</span></div><div class="line">  <span class="keyword">val</span> minCoresPerExecutor = coresPerExecutor.getOrElse(<span class="number">1</span>)</div><div class="line">  <span class="comment">// 如果用户没有设置 coresPerExecutor，那么 oneExecutorPerWorker 为 true</span></div><div class="line">  <span class="keyword">val</span> oneExecutorPerWorker = coresPerExecutor.isEmpty</div><div class="line">  <span class="comment">// 每个 executor 需要的 memory 用量</span></div><div class="line">  <span class="keyword">val</span> memoryPerExecutor = app.desc.memoryPerExecutorMB</div><div class="line">  <span class="comment">// 可用的 workers 个数</span></div><div class="line">  <span class="keyword">val</span> numUsable = usableWorkers.length</div><div class="line">  <span class="comment">// 每个 worker 要提供的 cores 个数</span></div><div class="line">  <span class="keyword">val</span> assignedCores = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](numUsable) <span class="comment">// Number of cores to give to each worker</span></div><div class="line">  <span class="comment">// 在每个 worker 上分配的 executor 个数</span></div><div class="line">  <span class="keyword">val</span> assignedExecutors = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](numUsable) <span class="comment">// Number of new executors on each worker</span></div><div class="line">  <span class="comment">// 要分配的 core 个数 = min(app 需求的 cores，workers 剩余 cores 之和)</span></div><div class="line">  <span class="keyword">var</span> coresToAssign = math.min(app.coresLeft, usableWorkers.map(_.coresFree).sum)</div><div class="line"></div><div class="line">  <span class="comment">/** Return whether the specified worker can launch an executor for this app. */</span></div><div class="line">  <span class="comment">// 从所有 workers 中筛选出可用的 workers，这就是筛选worker启动Executor的算法</span></div><div class="line">  <span class="comment">// 这个函数，后面作为filter的参数来调用，每次传入的是一个pos</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">canLaunchExecutor</span></span>(pos: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</div><div class="line">   <span class="comment">// 在后面单独有，因为确实太重要</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Keep launching executors until no more workers can accommodate any</span></div><div class="line">  <span class="comment">// more executors, or if we have reached this application's limits</span></div><div class="line">  <span class="comment">// 根据filter就过滤出来满足在Worker上launchExecutor的条件</span></div><div class="line">  <span class="keyword">var</span> freeWorkers = (<span class="number">0</span> until numUsable).filter(canLaunchExecutor)</div><div class="line">  <span class="comment">// 可用的Worker不是空的话，就执行下面的循环。</span></div><div class="line">  <span class="keyword">while</span> (freeWorkers.nonEmpty) &#123;</div><div class="line">    freeWorkers.foreach &#123; pos =&gt;</div><div class="line">      <span class="keyword">var</span> keepScheduling = <span class="literal">true</span></div><div class="line">      <span class="comment">// 如果该 worker 上可以启动 executor</span></div><div class="line">      <span class="keyword">while</span> (keepScheduling &amp;&amp; canLaunchExecutor(pos)) &#123;</div><div class="line">        <span class="comment">// 需要分配的 cores 的数目减去每个 executor 需要的 core 个数</span></div><div class="line">        <span class="comment">// minCoresPerExecutor 是用户设置的 spark.executor.cores</span></div><div class="line">        coresToAssign -= minCoresPerExecutor</div><div class="line">        <span class="comment">// 将分配好的 core 信息保存到 assignedCores 里面</span></div><div class="line">        assignedCores(pos) += minCoresPerExecutor</div><div class="line"></div><div class="line">        <span class="comment">// If we are launching one executor per worker, then every iteration assigns 1 core</span></div><div class="line">        <span class="comment">// to the executor. Otherwise, every iteration assigns cores to a new executor.</span></div><div class="line">          <span class="comment">/*</span></div><div class="line">如果是每个Worker下面只能够为当前的应用程序分配一个Executor的话，每次是分配一个Core!</div><div class="line">如果是spreadOutApps(也是系统默认的情况下)的时候，会尽量使用集群中所有的executors. 每次都会给executor增加一个core。</div><div class="line">如果不是spreadOutApps的时候，每次都会给executor增加一个core，会一直循环当前程序的executor上的freeCores,所以会占用本机器上的尽可能多的cores。</div><div class="line">           */</div><div class="line">          <span class="comment">// 若用户没有设置 spark.executor.cores</span></div><div class="line">          <span class="comment">// 则oneExecutorPerWorker就为True</span></div><div class="line">          <span class="comment">// 也就是说，assignedCores中的core都被一个executor使用</span></div><div class="line">          <span class="comment">// 若用户设置了spark.executor.cores，</span></div><div class="line">          <span class="comment">// 则该Worker的assignedExecutors会加1</span></div><div class="line">        <span class="keyword">if</span> (oneExecutorPerWorker) &#123;</div><div class="line">          assignedExecutors(pos) = <span class="number">1</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          assignedExecutors(pos) += <span class="number">1</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// Spreading out an application means spreading out its executors across as</span></div><div class="line">        <span class="comment">// many workers as possible. If we are not spreading out, then we should keep</span></div><div class="line">        <span class="comment">// scheduling executors on this worker until we use all of its resources.</span></div><div class="line">        <span class="comment">// Otherwise, just move on to the next worker.</span></div><div class="line">        <span class="comment">//如果不是spreadOutApps的话，会尽可能用当前的机器去处理程序的一切的cores需求，也就是executor会占用尽可能多的cores。</span></div><div class="line"><span class="comment">// 如果选择 spreadOut 模式，那么在一个 worker 上分配一个 executor 的 cores 后，就更换</span></div><div class="line"><span class="comment">// worker 再分配</span></div><div class="line">        </div><div class="line">          <span class="comment">//资源分配算法有两种：</span></div><div class="line">          <span class="comment">// 1. 尽量打散，将一个app尽可能的分配到不同的节点上，</span></div><div class="line">          <span class="comment">// 这有利于充分利用集群的资源，</span></div><div class="line">          <span class="comment">// 在这种情况下，spreadOutApps设为True，</span></div><div class="line">          <span class="comment">// 于是，该worker分配好了一个executor之后就退出循环</span></div><div class="line">          <span class="comment">// 轮询到下一个worker</span></div><div class="line">          <span class="comment">// 2. 尽量集中，将一个app尽可能的分配到同一个的节点上，</span></div><div class="line">          <span class="comment">// 这适合cpu密集型而内存占用比较少的app</span></div><div class="line">          <span class="comment">// 在这种情况下，spreadOutApps设为False，</span></div><div class="line">          <span class="comment">// 于是，继续下一轮的循环</span></div><div class="line">          <span class="comment">// 在该Worker上分配executor</span></div><div class="line">        <span class="keyword">if</span> (spreadOutApps) &#123;</div><div class="line">          keepScheduling = <span class="literal">false</span></div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 从 freeWorkers 中再挑选出可以启动 executor 的 workers</span></div><div class="line">    freeWorkers = freeWorkers.filter(canLaunchExecutor)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 返回在 workers 上分配的 CPU core 资源信息</span></div><div class="line">  assignedCores</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里单独把<code>canLaunchExecutor</code>拿出来，因为太重要了</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">canLaunchExecutor</span></span>(pos: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="comment">// 如果 app 里要分配的 core 个数大于每个 executor 需要的个数，仍然继续调度</span></div><div class="line">  <span class="keyword">val</span> keepScheduling = coresToAssign &gt;= minCoresPerExecutor</div><div class="line">  <span class="comment">// 当前 worker 是否有足够的 core 来分配</span></div><div class="line">  <span class="keyword">val</span> enoughCores = usableWorkers(pos).coresFree - assignedCores(pos) &gt;= minCoresPerExecutor</div><div class="line"></div><div class="line">  <span class="comment">// If we allow multiple executors per worker, then we can always launch new executors.</span></div><div class="line">  <span class="comment">// Otherwise, if there is already an executor on this worker, just give it more cores.</span></div><div class="line"> <span class="comment">// 条件3： 若设置了spark.executor.cores </span></div><div class="line"> <span class="comment">// 或者 该Worker还未分配executor</span></div><div class="line">  <span class="keyword">val</span> launchingNewExecutor = !oneExecutorPerWorker || assignedExecutors(pos) == <span class="number">0</span></div><div class="line">      </div><div class="line">  <span class="comment">// 如果每个 worker 可以分配多个 executor，或者这个 worker 上还没分配到 executor</span></div><div class="line">  <span class="keyword">if</span> (launchingNewExecutor) &#123;</div><div class="line">    <span class="comment">// 在不里，需要检查worker的空闲core和内存是否够用</span></div><div class="line">    <span class="comment">// 计算要在该 worker 上分配了多少 memory</span></div><div class="line">    <span class="keyword">val</span> assignedMemory = assignedExecutors(pos) * memoryPerExecutor</div><div class="line">    <span class="comment">// 计算该 worker 上是否有足够的 memory 来分配 executor</span></div><div class="line">    <span class="keyword">val</span> enoughMemory = usableWorkers(pos).memoryFree - assignedMemory &gt;= memoryPerExecutor</div><div class="line">    <span class="comment">// 检测是否超过 app executor 数目上限，用于动态调度</span></div><div class="line">    <span class="keyword">val</span> underLimit = assignedExecutors.sum + app.executors.size &lt; app.executorLimit</div><div class="line">    keepScheduling &amp;&amp; enoughCores &amp;&amp; enoughMemory &amp;&amp; underLimit</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// We're adding cores to an existing executor, so no need</span></div><div class="line">    <span class="comment">// to check memory and executor limits</span></div><div class="line">    <span class="comment">// 尤其需要注意的是，oneExecutorPerWorker机制下，不检测内存限制，很重要。</span></div><div class="line">    <span class="comment">// 如果每个 worker 只运行一个 executor，那么直接在该 executor 上增加 core 个数</span></div><div class="line">    keepScheduling &amp;&amp; enoughCores</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Spark1.4以后新的逻辑实现其实就是将分配单位从原来的一个core，变为了一个executor（即spark.executor.cores）。而若一个worker上只有一个executor（即没有设置spark.executor.cores），那么就按照原来的逻辑实现。</p>
<p><code>allocateWorkerResourceToExecutors</code>为Worker分配给定的Executor</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Allocate a worker's resources to one or more executors.</div><div class="line"> * @param app the info of the application which the executors belong to</div><div class="line"> * @param assignedCores number of cores on this worker for this application</div><div class="line"> * @param coresPerExecutor number of cores per executor</div><div class="line"> * @param worker the worker info</div><div class="line"> */</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">allocateWorkerResourceToExecutors</span></span>(</div><div class="line">    app: <span class="type">ApplicationInfo</span>,</div><div class="line">    assignedCores: <span class="type">Int</span>,</div><div class="line">    coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>],</div><div class="line">    worker: <span class="type">WorkerInfo</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="comment">// If the number of cores per executor is specified, we divide the cores assigned</span></div><div class="line">  <span class="comment">// to this worker evenly among the executors with no remainder.</span></div><div class="line">  <span class="comment">// Otherwise, we launch a single executor that grabs all the assignedCores on this worker.</span></div><div class="line">  <span class="comment">// 计算需要在该 worker 上启动多少个 executors (assignedCores / coresPerExecutor)</span></div><div class="line">  <span class="comment">// 该work上的executor数量，若没设置 spark.executor.cores，则为1</span></div><div class="line">  <span class="keyword">val</span> numExecutors = coresPerExecutor.map &#123; assignedCores / _ &#125;.getOrElse(<span class="number">1</span>)</div><div class="line">  <span class="comment">// 每个 executor 需要多少个 core</span></div><div class="line">  <span class="comment">// 分配给一个executor的core数量，若没设置 spark.executor.cores，则为该worker上所分配的所有core是数量</span></div><div class="line">  <span class="keyword">val</span> coresToAssign = coresPerExecutor.getOrElse(assignedCores)</div><div class="line">  <span class="comment">// 将 executor 信息加到 app 上，在 worker 上启动相应的 executor</span></div><div class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to numExecutors) &#123;</div><div class="line">    <span class="comment">//创建该executor信息，并把它加入到app信息中，并返回executor信息</span></div><div class="line">    <span class="keyword">val</span> exec = app.addExecutor(worker, coresToAssign)</div><div class="line">    launchExecutor(worker, exec)</div><div class="line">    app.state = <span class="type">ApplicationState</span>.<span class="type">RUNNING</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>launchExecutor</code>如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchExecutor</span></span>(worker: <span class="type">WorkerInfo</span>, exec: <span class="type">ExecutorDesc</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  logInfo(<span class="string">"Launching executor "</span> + exec.fullId + <span class="string">" on worker "</span> + worker.id)</div><div class="line">  <span class="comment">// 将Executor加入到worker的内存缓存</span></div><div class="line">  worker.addExecutor(exec)</div><div class="line">  <span class="comment">// 向worker发送LaunchExecutor消息</span></div><div class="line">  worker.endpoint.send(<span class="type">LaunchExecutor</span>(masterUrl,</div><div class="line">    exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))</div><div class="line">  exec.application.driver.send(</div><div class="line">    <span class="type">ExecutorAdded</span>(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<hr>
<p>Master中Application的调度机制（master的核心）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> spreadOutApps = conf.getBoolean(<span class="string">"spark.deploy.spreadOut"</span>, <span class="literal">true</span>)</div></pre></td></tr></table></figure>
<p>默认会使用spreadOut的调度算法进行app调度</p>
<hr>
<h2 id="Job出发流程"><a href="#Job出发流程" class="headerlink" title="Job出发流程"></a>Job出发流程</h2><p>每个action操作，会调用runJob方法，多次递归调用之后，会调用到DAGScheduler的runJob方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</div></pre></td></tr></table></figure>
<p>DAGScheduler的stage划分算法：会从出发action 的操作往前倒推，首先就是为最后一个rdd创建一个finalStage，然后往前倒推的时候，如果发现某个rdd是宽依赖，那么就会为宽依赖的rdd创建一个新的stage，这个rdd就是新stage的最后一个rdd，直到所有的rdd遍历完毕。</p>
<p>DAGScheduler的runJob方法最后会将job提交到<code>DAGSchedulerEventProcessLoop</code>，这里会调用到doOnReceive方法，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</div><div class="line">    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</div></pre></td></tr></table></figure>
<p><code>handleJobSubmitted</code>方法</p>
<p>该方法首先使用出发Job的finalRDD创建ResultStage</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></div><div class="line">    <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></div><div class="line">    finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">      logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)</div><div class="line">      listener.jobFailed(e)</div><div class="line">      <span class="keyword">return</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>然后就是设置一些job的参数，比如jobID，job提交时间等等</p>
<p>最后提交stage，submitStage</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 这个方法的调用，会导致第一个stage的提交，并且导致其他所有的stage的放入到了waitingStages队列中</span></div><div class="line">submitStage(finalStage)</div><div class="line"></div><div class="line"><span class="comment">// 提交等待的stage</span></div><div class="line">submitWaitingStages()</div></pre></td></tr></table></figure>
<p><code>submitStage</code>是stage划分算法的入口，由submitStage和getMissingParentStages方法一起组成的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * stage划分算法的入口</div><div class="line">  * stage划分算法 由submitStage和getMissingParentStages方法一起组成的</div><div class="line">  */</div><div class="line"><span class="comment">/** Submits stage, but first recursively submits any missing parents. */</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</div><div class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</div><div class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</div><div class="line">    logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>)</div><div class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</div><div class="line">      <span class="comment">//getMissingParentStages 方法获取当前stage的父stage</span></div><div class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</div><div class="line">      logDebug(<span class="string">"missing: "</span> + missing)</div><div class="line">      <span class="comment">// 如果没有父stage，则stage划分完毕，提交job</span></div><div class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</div><div class="line">        logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</div><div class="line">        submitMissingTasks(stage, jobId.get)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 递归调用submitStage方法提交stage</span></div><div class="line">        <span class="comment">// 每次递归提交父stage，会触发stage的划分</span></div><div class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</div><div class="line">          submitStage(parent)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// 将当前stage放入等待队列中</span></div><div class="line">        waitingStages += stage</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>getMissingParentStages用来获取当前stage的父stage</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * 获取某个stage的父stage</div><div class="line">   * 对于一个stage，如果它的只有一个rdd的所有依赖，都是窄依赖，那么不会创建任何新的stage</div><div class="line">   * 但是，如果这个stage的rdd宽依赖了某个rdd，那么就是用宽依赖的rdd创建一个新的stage</div><div class="line">   */</div><div class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</div><div class="line">   <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</div><div class="line">   <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</div><div class="line">   <span class="comment">// We are manually maintaining a stack here to prevent StackOverflowError</span></div><div class="line">   <span class="comment">// caused by recursively visiting</span></div><div class="line">   <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</div><div class="line">     <span class="keyword">if</span> (!visited(rdd)) &#123;</div><div class="line">       visited += rdd</div><div class="line">       <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</div><div class="line">       <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</div><div class="line">         <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</div><div class="line">           dep <span class="keyword">match</span> &#123;</div><div class="line">             <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</div><div class="line">               <span class="keyword">val</span> mapStage = getShuffleMapStage(shufDep, stage.firstJobId)</div><div class="line">               <span class="keyword">if</span> (!mapStage.isAvailable) &#123;</div><div class="line">                 missing += mapStage</div><div class="line">               &#125;</div><div class="line">             <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;</div><div class="line">               waitingForVisit.push(narrowDep.rdd)</div><div class="line">           &#125;</div><div class="line">         &#125;</div><div class="line">       &#125;</div><div class="line">     &#125;</div><div class="line">   &#125;</div><div class="line">   waitingForVisit.push(stage.rdd)</div><div class="line">   <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</div><div class="line">     visit(waitingForVisit.pop())</div><div class="line">   &#125;</div><div class="line">   missing.toList</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>上述是stage划分算法的核心，会遍历rdd的父rdd，如果遇到宽依赖，则创建新的stage，如果是窄依赖，则继续递归。</p>
<p>之后回到<code>org.apache.spark.scheduler.DAGScheduler#submitStage</code>里，如果当前stage没有父stage，则会触发提交</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (missing.isEmpty) &#123;</div><div class="line">  logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</div><div class="line">  submitMissingTasks(stage, jobId.get)</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">  <span class="comment">// 递归调用submitStage方法提交stage</span></div><div class="line">  <span class="comment">// 每次递归提交父stage，会触发stage的划分</span></div><div class="line">  <span class="keyword">for</span> (parent &lt;- missing) &#123;</div><div class="line">    submitStage(parent)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 将当前stage放入等待队列中</span></div><div class="line">  waitingStages += stage</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>org.apache.spark.scheduler.DAGScheduler#submitMissingTasks</code>这个方法，意味着stage划分完毕，开始创建task，为每个partition都创建一个task</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">    * 此处为stage创建指定数量tasks</div><div class="line">    */</div><div class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</div><div class="line">    stage <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</div><div class="line">        partitionsToCompute.map &#123; id =&gt;</div><div class="line">          <span class="comment">// 计算每个task的最佳位置，这个很重要</span></div><div class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</div><div class="line">          <span class="keyword">val</span> part = stage.rdd.partitions(id)</div><div class="line">          <span class="comment">// 创建shuffleMapTask</span></div><div class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptId,</div><div class="line">            taskBinary, part, locs, stage.internalAccumulators)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</div><div class="line">        <span class="keyword">val</span> job = stage.activeJob.get</div><div class="line">        partitionsToCompute.map &#123; id =&gt;</div><div class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</div><div class="line">          <span class="keyword">val</span> part = stage.rdd.partitions(p)</div><div class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</div><div class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptId,</div><div class="line">            taskBinary, part, locs, id, stage.internalAccumulators)</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>特别的，这里有个方法，计算每个task的最佳位置。taskIdToLocations是一个map，value就是每个partitionID的TaskLocation位置，会调用getPreferredLocs计算最佳位置</p>
<p>org.apache.spark.scheduler.DAGScheduler#getPreferredLocs，最终调用到了getPreferredLocsInternal方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line">   * Recursive implementation for getPreferredLocs.</div><div class="line">   *</div><div class="line">   * This method is thread-safe because it only accesses DAGScheduler state through thread-safe</div><div class="line">   * methods (getCacheLocs()); please be careful when modifying this method, because any new</div><div class="line">   * DAGScheduler state accessed by it may require additional synchronization.</div><div class="line">   */</div><div class="line">  <span class="comment">/**</span></div><div class="line">    * 计算task的最佳位置</div><div class="line">    * 就是，从stage的最后一个rdd开始，那个stage的partition被cache或者checkpoint了，那么，task的最佳位置就是执行缓存的位置</div><div class="line">    */</div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocsInternal</span></span>(</div><div class="line">      rdd: <span class="type">RDD</span>[_],</div><div class="line">      partition: <span class="type">Int</span>,</div><div class="line">      visited: <span class="type">HashSet</span>[(<span class="type">RDD</span>[_], <span class="type">Int</span>)]): <span class="type">Seq</span>[<span class="type">TaskLocation</span>] = &#123;</div><div class="line">    <span class="comment">// If the partition has already been visited, no need to re-visit.</span></div><div class="line">    <span class="comment">// This avoids exponential path exploration.  SPARK-695</span></div><div class="line">    <span class="keyword">if</span> (!visited.add((rdd, partition))) &#123;</div><div class="line">      <span class="comment">// Nil has already been returned for previously visited partitions.</span></div><div class="line">      <span class="keyword">return</span> <span class="type">Nil</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">// If the partition is cached, return the cache locations</span></div><div class="line">    <span class="keyword">val</span> cached = getCacheLocs(rdd)(partition)</div><div class="line">    <span class="keyword">if</span> (cached.nonEmpty) &#123;</div><div class="line">      <span class="keyword">return</span> cached</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// If the RDD has some placement preferences (as is the case for input RDDs), get those</span></div><div class="line">    <span class="keyword">val</span> rddPrefs = rdd.preferredLocations(rdd.partitions(partition)).toList</div><div class="line">    <span class="keyword">if</span> (rddPrefs.nonEmpty) &#123;</div><div class="line">      <span class="keyword">return</span> rddPrefs.map(<span class="type">TaskLocation</span>(_))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// If the RDD has narrow dependencies, pick the first partition of the first narrow dependency</span></div><div class="line">    <span class="comment">// that has any placement preferences. Ideally we would choose based on transfer sizes,</span></div><div class="line">    <span class="comment">// but this will do for now.</span></div><div class="line">    <span class="comment">// 递归调用rdd的父rdd，如果有缓存或者checkpoint，则返回该位置，否则为Nil</span></div><div class="line">    rdd.dependencies.foreach &#123;</div><div class="line">      <span class="keyword">case</span> n: <span class="type">NarrowDependency</span>[_] =&gt;</div><div class="line">        <span class="keyword">for</span> (inPart &lt;- n.getParents(partition)) &#123;</div><div class="line">          <span class="keyword">val</span> locs = getPreferredLocsInternal(n.rdd, inPart, visited)</div><div class="line">          <span class="keyword">if</span> (locs != <span class="type">Nil</span>) &#123;</div><div class="line">            <span class="keyword">return</span> locs</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> _ =&gt;</div><div class="line">    &#125;</div><div class="line"><span class="comment">// 没有checkpoint或者cache，则返回Nil</span></div><div class="line">    <span class="type">Nil</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>此处会递归调用，判断rdd的partition以及父partition时候被cache或者checkpoint，如果是，则直接返回该位置，否则为Nil</p>
<p>回到<code>org.apache.spark.scheduler.DAGScheduler#submitMissingTasks</code>方法中，在创建完task之后，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</div><div class="line">     logInfo(<span class="string">"Submitting "</span> + tasks.size + <span class="string">" missing tasks from "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">")"</span>)</div><div class="line">     stage.pendingPartitions ++= tasks.map(_.partitionId)</div><div class="line">     logDebug(<span class="string">"New pending partitions: "</span> + stage.pendingPartitions)</div><div class="line">     taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</div><div class="line">       tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))</div><div class="line">     stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div></pre></td></tr></table></figure>
<p>TaskSet保存了Stage包含的一组完全相同的Task，每个Task的处理逻辑完全相同，不同的是处理的数据，每个Task负责一个Partition。</p>
<p>会使用taskScheduler提交tasks，submitTasks</p>
<p><code>org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks</code>这个用来提交tasks</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>) &#123;</div><div class="line">  <span class="keyword">val</span> tasks = taskSet.tasks</div><div class="line">  logInfo(<span class="string">"Adding task set "</span> + taskSet.id + <span class="string">" with "</span> + tasks.length + <span class="string">" tasks"</span>)</div><div class="line">  <span class="keyword">this</span>.synchronized &#123;</div><div class="line">    <span class="comment">// 生成TaskSetManager</span></div><div class="line">    <span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</div><div class="line">    <span class="keyword">val</span> stage = taskSet.stageId</div><div class="line">    <span class="keyword">val</span> stageTaskSets =</div><div class="line">      taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">TaskSetManager</span>])</div><div class="line">    stageTaskSets(taskSet.stageAttemptId) = manager</div><div class="line">    <span class="keyword">val</span> conflictingTaskSet = stageTaskSets.exists &#123; <span class="keyword">case</span> (_, ts) =&gt;</div><div class="line">      ts.taskSet != taskSet &amp;&amp; !ts.isZombie</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (conflictingTaskSet) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"more than one active taskSet for stage <span class="subst">$stage</span>:"</span> +</div><div class="line">        <span class="string">s" <span class="subst">$&#123;stageTaskSets.toSeq.map&#123;_._2.taskSet.id&#125;</span>.mkString("</span>,<span class="string">")&#125;"</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 将manager等信息放入调度器</span></div><div class="line">    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (!isLocal &amp;&amp; !hasReceivedTask) &#123;</div><div class="line">      starvationTimer.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">TimerTask</span>() &#123;</div><div class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">          <span class="keyword">if</span> (!hasLaunchedTask) &#123;</div><div class="line">            logWarning(<span class="string">"Initial job has not accepted any resources; "</span> +</div><div class="line">              <span class="string">"check your cluster UI to ensure that workers are registered "</span> +</div><div class="line">              <span class="string">"and have sufficient resources"</span>)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">this</span>.cancel()</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;, <span class="type">STARVATION_TIMEOUT_MS</span>, <span class="type">STARVATION_TIMEOUT_MS</span>)</div><div class="line">    &#125;</div><div class="line">    hasReceivedTask = <span class="literal">true</span></div><div class="line">  &#125;</div><div class="line">  backend.reviveOffers()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>TaskSetManager会标记当前的taskset以及当前task重试次数</p>
<p>最后会将TaskSetManager提交阿斗调度器调度</p>
<p>共有两种调度方法：FIFO和FAIR。这个调度器是在SparkContext构建的时候就初始化了。调度算法后面再讨论。</p>
<p>注意最后一句，backend.reviveOffers()，这是关键。backend在SparkContext初始化的时候决定了，暂时讨论Spark Standalone模式的</p>
<p><code>reviveOffers</code>底层调用到了<code>org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend#reviveOffers</code>，这个其实就是给自己rpc发消息</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>() &#123;</div><div class="line">  driverEndpoint.send(<span class="type">ReviveOffers</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后，receive()，<code>org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#receive</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</div><div class="line">    scheduler.statusUpdate(taskId, state, data.value)</div><div class="line">    <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</div><div class="line">      executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</div><div class="line">          executorInfo.freeCores += scheduler.<span class="type">CPUS_PER_TASK</span></div><div class="line">          makeOffers(executorId)</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// Ignoring the update since we don't know about the executor.</span></div><div class="line">          logWarning(<span class="string">s"Ignored task status update (<span class="subst">$taskId</span> state <span class="subst">$state</span>) "</span> +</div><div class="line">            <span class="string">s"from unknown executor with ID <span class="subst">$executorId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 就是调用到了makeOffers</span></div><div class="line">  <span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</div><div class="line">    makeOffers()</div><div class="line"></div><div class="line">  <span class="keyword">case</span> <span class="type">KillTask</span>(taskId, executorId, interruptThread) =&gt;</div><div class="line">    executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</div><div class="line">        executorInfo.executorEndpoint.send(<span class="type">KillTask</span>(taskId, executorId, interruptThread))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">// Ignoring the task kill since the executor is not registered.</span></div><div class="line">        logWarning(<span class="string">s"Attempted to kill task <span class="subst">$taskId</span> for unknown executor <span class="subst">$executorId</span>."</span>)</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Make fake resource offers on all executors</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeOffers</span></span>() &#123;</div><div class="line">  <span class="comment">// Filter out executors under killing</span></div><div class="line">  <span class="comment">// 过滤掉被杀死的Executor</span></div><div class="line">  <span class="keyword">val</span> activeExecutors = executorDataMap.filterKeys(executorIsAlive)</div><div class="line">  <span class="comment">// 根据activeExecutors生成workOffers，</span></div><div class="line">  <span class="comment">// 即executor所能提供的资源信息。</span></div><div class="line">  <span class="keyword">val</span> workOffers = activeExecutors.map &#123; <span class="keyword">case</span> (id, executorData) =&gt;</div><div class="line">    <span class="keyword">new</span> <span class="type">WorkerOffer</span>(id, executorData.executorHost, executorData.freeCores)</div><div class="line">  &#125;.toSeq</div><div class="line">  <span class="comment">// scheduler.resourceOffers分配资源，</span></div><div class="line">  <span class="comment">// 并launchTasks发送任务</span></div><div class="line">  launchTasks(scheduler.resourceOffers(workOffers))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>launchTasks主要的实现是向executor发送LaunchTask信号。</p>
<p>下面重点说一下<code>scheduler.resourceOffers()</code>方法，决定了每个task运行的节点</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resourceOffers</span></span>(offers: <span class="type">IndexedSeq</span>[<span class="type">WorkerOffer</span>]): <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]] = synchronized &#123;</div><div class="line">  <span class="comment">// 标记每个活的节点并记录它的主机名</span></div><div class="line">  <span class="comment">// 并且追踪是否有新的executor加入</span></div><div class="line">  <span class="keyword">var</span> newExecAvail = <span class="literal">false</span></div><div class="line">  <span class="keyword">for</span> (o &lt;- offers) &#123;</div><div class="line">    <span class="keyword">if</span> (!hostToExecutors.contains(o.host)) &#123;</div><div class="line">      hostToExecutors(o.host) = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]()</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!executorIdToRunningTaskIds.contains(o.executorId)) &#123;</div><div class="line">      hostToExecutors(o.host) += o.executorId</div><div class="line">      executorAdded(o.executorId, o.host)</div><div class="line">      executorIdToHost(o.executorId) = o.host</div><div class="line">      executorIdToRunningTaskIds(o.executorId) = <span class="type">HashSet</span>[<span class="type">Long</span>]()</div><div class="line">      newExecAvail = <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span> (rack &lt;- getRackForHost(o.host)) &#123;</div><div class="line">      hostsByRack.getOrElseUpdate(rack, <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]()) += o.host</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 为了避免将Task集中分配到某些机器，随机的打散它们</span></div><div class="line">  <span class="keyword">val</span> shuffledOffers = <span class="type">Random</span>.shuffle(offers)</div><div class="line">  <span class="comment">// 建立每个worker的TaskDescription数组</span></div><div class="line">  <span class="keyword">val</span> tasks = shuffledOffers.map(o =&gt; <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">TaskDescription</span>](o.cores))</div><div class="line">  <span class="comment">// 记录各个worker的available Cpus</span></div><div class="line">  <span class="keyword">val</span> availableCpus = shuffledOffers.map(o =&gt; o.cores).toArray</div><div class="line">  <span class="comment">// 获取按照调度策略排序好的TaskSetManager</span></div><div class="line">  <span class="keyword">val</span> sortedTaskSets = rootPool.getSortedTaskSetQueue</div><div class="line">  <span class="keyword">for</span> (taskSet &lt;- sortedTaskSets) &#123;</div><div class="line">    logDebug(<span class="string">"parentName: %s, name: %s, runningTasks: %s"</span>.format(</div><div class="line">      taskSet.parent.name, taskSet.name, taskSet.runningTasks))</div><div class="line">      <span class="comment">//如果有新的executor加入</span></div><div class="line">      <span class="comment">//则需要从新计算TaskSetManager的就近原则</span></div><div class="line">    <span class="keyword">if</span> (newExecAvail) &#123;</div><div class="line">      taskSet.executorAdded()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 得到调度序列中的每个TaskSet,</span></div><div class="line">  <span class="comment">// 然后按节点的locality级别增序分配资源</span></div><div class="line">  <span class="comment">// Locality优先序列为: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY</span></div><div class="line">  <span class="keyword">for</span> (taskSet &lt;- sortedTaskSets) &#123;</div><div class="line">    <span class="keyword">var</span> launchedAnyTask = <span class="literal">false</span></div><div class="line">    <span class="keyword">var</span> launchedTaskAtCurrentMaxLocality = <span class="literal">false</span></div><div class="line">    <span class="comment">//按照就近原则分配</span></div><div class="line">    <span class="keyword">for</span> (currentMaxLocality &lt;- taskSet.myLocalityLevels) &#123;</div><div class="line">      do &#123;</div><div class="line">      <span class="comment">// resourceOfferSingleTaskSet为单个TaskSet分配资源，</span></div><div class="line">      <span class="comment">// 若该LocalityLevel的节点下不能再为之分配资源了，</span></div><div class="line">      <span class="comment">// 则返回false</span></div><div class="line">        launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet(</div><div class="line">          taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)</div><div class="line">        launchedAnyTask |= launchedTaskAtCurrentMaxLocality</div><div class="line">      &#125; <span class="keyword">while</span> (launchedTaskAtCurrentMaxLocality)</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!launchedAnyTask) &#123;</div><div class="line">      taskSet.abortIfCompletelyBlacklisted(hostToExecutors)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</div><div class="line">    hasLaunchedTask = <span class="literal">true</span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> tasks</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>​    <code>val sortedTaskSets = rootPool.getSortedTaskSetQueue</code> 这个就是task调度之后给我们的task</p>
<p><code>org.apache.spark.scheduler.TaskSchedulerImpl#resourceOfferSingleTaskSet</code>这个就是为每个task寻找Executor的过程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">resourceOfferSingleTaskSet</span></span>(</div><div class="line">     taskSet: <span class="type">TaskSetManager</span>,</div><div class="line">     maxLocality: <span class="type">TaskLocality</span>,</div><div class="line">     shuffledOffers: <span class="type">Seq</span>[<span class="type">WorkerOffer</span>],</div><div class="line">     availableCpus: <span class="type">Array</span>[<span class="type">Int</span>],</div><div class="line">     tasks: <span class="type">IndexedSeq</span>[<span class="type">ArrayBuffer</span>[<span class="type">TaskDescription</span>]]) : <span class="type">Boolean</span> = &#123;</div><div class="line">   <span class="keyword">var</span> launchedTask = <span class="literal">false</span></div><div class="line">   <span class="comment">//遍历各个executor</span></div><div class="line">   <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until shuffledOffers.size) &#123;</div><div class="line">     <span class="keyword">val</span> execId = shuffledOffers(i).executorId</div><div class="line">     <span class="keyword">val</span> host = shuffledOffers(i).host</div><div class="line">     <span class="keyword">if</span> (availableCpus(i) &gt;= <span class="type">CPUS_PER_TASK</span>) &#123;</div><div class="line">       <span class="keyword">try</span> &#123;</div><div class="line">         <span class="comment">//获取taskSet中，相对于该execId, host所能接收的最大距离maxLocality的task</span></div><div class="line">         <span class="comment">//maxLocality的值在TaskSchedulerImpl.resourceOffers中从近到远的遍历</span></div><div class="line">         <span class="keyword">for</span> (task &lt;- taskSet.resourceOffer(execId, host, maxLocality)) &#123;</div><div class="line">           tasks(i) += task</div><div class="line">           <span class="keyword">val</span> tid = task.taskId</div><div class="line">           taskIdToTaskSetManager(tid) = taskSet</div><div class="line">           taskIdToExecutorId(tid) = execId</div><div class="line">           executorIdToRunningTaskIds(execId).add(tid)</div><div class="line">           availableCpus(i) -= <span class="type">CPUS_PER_TASK</span></div><div class="line">           assert(availableCpus(i) &gt;= <span class="number">0</span>)</div><div class="line">           launchedTask = <span class="literal">true</span></div><div class="line">         &#125;</div><div class="line">       &#125; <span class="keyword">catch</span> &#123;</div><div class="line">         <span class="keyword">case</span> e: <span class="type">TaskNotSerializableException</span> =&gt;</div><div class="line">           logError(<span class="string">s"Resource offer failed, task set <span class="subst">$&#123;taskSet.name&#125;</span> was not serializable"</span>)</div><div class="line">           <span class="keyword">return</span> launchedTask</div><div class="line">       &#125;</div><div class="line">     &#125;</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">return</span> launchedTask</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>这里会考虑每个task的本地行级别：</p>
<ul>
<li><code>PROCESS_LOCAL</code>: 数据在同一个 JVM 中，即同一个 executor 上。这是最佳数据 locality。</li>
<li><code>NODE_LOCAL</code>: 数据在同一个节点上。比如数据在同一个节点的另一个 executor上；或在 HDFS 上，恰好有 block 在同一个节点上。速度比 PROCESS_LOCAL 稍慢，因为数据需要在不同进程之间传递或从文件中读取</li>
<li><code>NO_PREF</code>: 数据从哪里访问都一样快，不需要位置优先</li>
<li><code>RACK_LOCAL</code>: 数据在同一机架的不同节点上。需要通过网络传输数据及文件 IO，比 NODE_LOCAL 慢</li>
<li><code>ANY</code>: 数据在非同一机架的网络上，速度最慢</li>
</ul>
<hr>
<ul>
<li>若 taskSet 中有 task 的 partition 是存储在 executor 内存中的且对应 executor alive，那么该 taskSet 的最佳本地性为 <code>PROCESS_LOCAL</code>,可用本地性集合包括 <code>PROCESS_LOCAL</code> 及所有本地性比 <code>PROCESS_LOCAL</code> 查的，也就是该集合包括 <code>PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY</code></li>
<li>若 taskSet 中没有 task 的 partition 是存储在 executor 内存中的，但存在 partition 是存储在某个节点磁盘上的且对应节点 alive ，那么该 taskSet 的最佳本地性为 <code>NODE_LOCAL</code>,可用本地性集合包括 <code>NODE_LOCAL</code> 及所有本地性比 <code>NODE_LOCAL</code> 查的，也就是该集合包括 <code>NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY</code></li>
<li>以此类推，可用本地性集合包含 taskSet 中的 tasks 所拥有的最佳本地性及所有比该本地性差的本地性</li>
</ul>
<p>回到<code>org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#makeOffers</code> launchTask过程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]) &#123;</div><div class="line">  <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</div><div class="line">    <span class="comment">// 序列化task</span></div><div class="line">    <span class="keyword">val</span> serializedTask = ser.serialize(task)</div><div class="line">    <span class="comment">//若序列话Task大小达到Rpc限制，</span></div><div class="line">    <span class="comment">//则停止</span></div><div class="line">    <span class="keyword">if</span> (serializedTask.limit &gt;= akkaFrameSize - <span class="type">AkkaUtils</span>.reservedSizeBytes) &#123;</div><div class="line">      scheduler.taskIdToTaskSetManager.get(task.taskId).foreach &#123; taskSetMgr =&gt;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">var</span> msg = <span class="string">"Serialized task %s:%d was %d bytes, which exceeds max allowed: "</span> +</div><div class="line">            <span class="string">"spark.akka.frameSize (%d bytes) - reserved (%d bytes). Consider increasing "</span> +</div><div class="line">            <span class="string">"spark.akka.frameSize or using broadcast variables for large values."</span></div><div class="line">          msg = msg.format(task.taskId, task.index, serializedTask.limit, akkaFrameSize,</div><div class="line">            <span class="type">AkkaUtils</span>.reservedSizeBytes)</div><div class="line">          taskSetMgr.abort(msg)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">"Exception in error callback"</span>, e)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">     <span class="comment">// 减少改task所对应的executor信息的core数量</span></div><div class="line">      <span class="keyword">val</span> executorData = executorDataMap(task.executorId)</div><div class="line">      executorData.freeCores -= scheduler.<span class="type">CPUS_PER_TASK</span></div><div class="line">      <span class="comment">//向executorEndpoint 发送LaunchTask 信号</span></div><div class="line">      executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，再来看一下两种task调度方法：略</p>
<hr>
<h2 id="Executor启动流程"><a href="#Executor启动流程" class="headerlink" title="Executor启动流程"></a>Executor启动流程</h2><ol>
<li>Worker在接收到launchExecutor消息后，会启动CoarseGrainedExecutorBackend进程，会立即向Driver反向注册，发送RegisterExecutor消息。Driver注册成功之后，会返回RegisteredExecutor消息</li>
<li>CoarseGrainedExecutorBackend收到RegisteredExecutor消息，会创建一个Executor对象</li>
<li>driver内的TaskScheduler向CoarseGrainedExecutorBackend发送launchTask消息，启动task</li>
<li>会调用Executor的launchTask方法，在launchTask中创建TaskRunner线程，将task丢进线程池。</li>
</ol>
<hr>
<h2 id="Task运行流程"><a href="#Task运行流程" class="headerlink" title="Task运行流程"></a>Task运行流程</h2><ol>
<li>Executor接受到LaunchTasks之后，会做一些准备操作，如反序列化Task，拉取需要的文件，jar包等</li>
<li>之后使用task的run()方法，运行。其中，最为核心的就是调用RDD的iterator()方法，针对task对应的rdd的partition，执行我们自定义的算子</li>
<li>Task分为两种：ShuffleMapTask和ResultTask<ol>
<li>如果是ShuffleMapTask，那么计算完partition上数据之后，会使用ShuffleManager的ShuffleWriter，将数据分区之后写入对应的分区文件，然后将MapStatus发送给Driver中的DAGScheduler的MapOutputTracker</li>
<li>如果是ResultTask，那么回去Driver上MapOutputTracker中找到对应的分区，去拉去数据计算。</li>
</ol>
</li>
</ol>
<h2 id="BlockManager"><a href="#BlockManager" class="headerlink" title="BlockManager"></a>BlockManager</h2>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/14/2017-05-27Java线程池/" rel="next" title="Java线程池">
                <i class="fa fa-chevron-left"></i> Java线程池
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/18/2017-06-18Spark Scheduler内部原理剖析/" rel="prev" title="Spark Scheduler内部原理剖析">
                Spark Scheduler内部原理剖析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="iclouding" />
          <p class="site-author-name" itemprop="name">iclouding</p>
           
              <p class="site-description motion-element" itemprop="description">Hadoop, Spark, OpenStack，Docker</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/iclouding" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.cnblogs.com/icloud/" target="_blank" title="博客园">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  博客园
                </a>
              </span>
            
          
        </div>

        
        

        
        
        
        
    <div class="links-of-blogroll motion-element links-of-blogroll-block">
      <div class="links-of-blogroll-title">
        <!-- modify icon to fire by szw -->
        <i class="fa fa-history fa-" aria-hidden="true"></i>
        近期文章
      </div>
          <ul class="links-of-blogroll-list">
            
            
              <li>
                <a href="/2020/01/01/2017-05-27博客收录/" title="博客收录" target="_blank">博客收录</a>
              </li>
            
              <li>
                <a href="/2018/05/29/2018-05-29Flume避坑指南/" title="Flume避坑指南" target="_blank">Flume避坑指南</a>
              </li>
            
              <li>
                <a href="/2018/05/22/2018-05-22Flume-tailDir-Source改进/" title="Flume tailDir Source改进" target="_blank">Flume tailDir Source改进</a>
              </li>
            
              <li>
                <a href="/2018/05/22/2018-05-22Flume日志收集工具/" title="Flume日志收集工具" target="_blank">Flume日志收集工具</a>
              </li>
            
              <li>
                <a href="/2018/05/16/2018-05-16Hive-UDF和UDAF开发/" title="Hive UDF和UDAF开发" target="_blank">Hive UDF和UDAF开发</a>
              </li>
            
              </ul>
            </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-源码"><span class="nav-number">1.</span> <span class="nav-text">Spark 源码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#整体流程"><span class="nav-number">1.1.</span> <span class="nav-text">整体流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkContext分析"><span class="nav-number">1.2.</span> <span class="nav-text">SparkContext分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Master原理"><span class="nav-number">1.3.</span> <span class="nav-text">Master原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Job出发流程"><span class="nav-number">1.4.</span> <span class="nav-text">Job出发流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Executor启动流程"><span class="nav-number">1.5.</span> <span class="nav-text">Executor启动流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task运行流程"><span class="nav-number">1.6.</span> <span class="nav-text">Task运行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BlockManager"><span class="nav-number">1.7.</span> <span class="nav-text">BlockManager</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">iclouding</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e5e46c90da2d400e9a09f945448bb299",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  










  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("ExxAsfadHnGbfXNeH3Y3EoEQ-gzGzoHsz", "scr6biItx0UTfQik4OWLoR36");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.1"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.1"></script>


  

</body>
</html>
